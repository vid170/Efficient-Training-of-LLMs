{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Group 25\n",
        "Shaleen Malik(12041360), Prarabdh Shukla(12041090), Vidhi Mittal(12041730), Vanisha Agrawal(12041670)"
      ],
      "metadata": {
        "id": "tkeGYPNSvp5S"
      },
      "id": "tkeGYPNSvp5S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "w5swoQJag_Yn"
      },
      "id": "w5swoQJag_Yn"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0KfnUI2aumj",
        "outputId": "789cd083-afb7-40a9-8521-ca9e27817505"
      },
      "id": "y0KfnUI2aumj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2MvYTdVwe2",
        "outputId": "c136c243-2ddf-4440-c974-84d048fccae9"
      },
      "id": "tV2MvYTdVwe2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-27 05:45:14--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.37, 3.163.189.114, 3.163.189.74, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/26cf7605aca15bc4ea6fa637256400d9d01317b28ed296172b2d1dd160cd7699?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories_all_data.tar.gz%3B+filename%3D%22TinyStories_all_data.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1711777514&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMTc3NzUxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxLzI2Y2Y3NjA1YWNhMTViYzRlYTZmYTYzNzI1NjQwMGQ5ZDAxMzE3YjI4ZWQyOTYxNzJiMmQxZGQxNjBjZDc2OTk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=HkqYATXTZMB9msBWNPmmo3nBSz-hGNtN1qmnU1EUVQ7f-dy8%7EIcMC9UIrxDgY2Xbe71HqZzxtNLYeXhwrUXSH1CjJrKq1AOTdQY570Hg0Bf5Q01Kn7tnmchDCd4HDwjIps3cYonv2C-a4Hjs23QXtPjFlozsCYqlwufmEbsL09-9g3%7EQEXrrTIqEZnZh4HNKtHTpkw-zV%7EVwx-UR8LhXFtwG8%7EPqBi%7EUAaL%7EhY9NBfpqjFGnajiSNakkNI6NYefcnkNXqD9oDZ-FgLnicxLwJ8arbx1XnuRSV9BxIOyuJnjyifdVLvhY9eyeuOXubQ%7EG%7EIBJn7pgGvMA6bBR9iYglw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-03-27 05:45:14--  https://cdn-lfs.huggingface.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/26cf7605aca15bc4ea6fa637256400d9d01317b28ed296172b2d1dd160cd7699?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories_all_data.tar.gz%3B+filename%3D%22TinyStories_all_data.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1711777514&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMTc3NzUxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxLzI2Y2Y3NjA1YWNhMTViYzRlYTZmYTYzNzI1NjQwMGQ5ZDAxMzE3YjI4ZWQyOTYxNzJiMmQxZGQxNjBjZDc2OTk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=HkqYATXTZMB9msBWNPmmo3nBSz-hGNtN1qmnU1EUVQ7f-dy8%7EIcMC9UIrxDgY2Xbe71HqZzxtNLYeXhwrUXSH1CjJrKq1AOTdQY570Hg0Bf5Q01Kn7tnmchDCd4HDwjIps3cYonv2C-a4Hjs23QXtPjFlozsCYqlwufmEbsL09-9g3%7EQEXrrTIqEZnZh4HNKtHTpkw-zV%7EVwx-UR8LhXFtwG8%7EPqBi%7EUAaL%7EhY9NBfpqjFGnajiSNakkNI6NYefcnkNXqD9oDZ-FgLnicxLwJ8arbx1XnuRSV9BxIOyuJnjyifdVLvhY9eyeuOXubQ%7EG%7EIBJn7pgGvMA6bBR9iYglw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.23, 108.138.94.14, 108.138.94.122, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608001638 (1.5G) [application/gzip]\n",
            "Saving to: ‘TinyStories_all_data.tar.gz’\n",
            "\n",
            "TinyStories_all_dat 100%[===================>]   1.50G  38.5MB/s    in 37s     \n",
            "\n",
            "2024-03-27 05:45:51 (41.6 MB/s) - ‘TinyStories_all_data.tar.gz’ saved [1608001638/1608001638]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf /content/TinyStories_all_data.tar.gz -C TinyStories"
      ],
      "metadata": {
        "id": "Taa919ypWI4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f551eba-8fce-4051-ca70-5b2ecd5dfd97"
      },
      "id": "Taa919ypWI4G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: TinyStories: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "ZKuW2rbWW7G-"
      },
      "id": "ZKuW2rbWW7G-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "FMm-_Wlmhf_L"
      },
      "id": "FMm-_Wlmhf_L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting data\n"
      ],
      "metadata": {
        "id": "00wofm0xhB7y"
      },
      "id": "00wofm0xhB7y"
    },
    {
      "cell_type": "code",
      "source": [
        "foo = open(\"/content/TinyStories-valid.txt\", \"r\")\n",
        "content = foo.readlines()\n",
        "con_str = ''\n",
        "for c in content:\n",
        "  con_str += c\n",
        "\n",
        "stories = con_str.split(\"<|endoftext|>\\n\")"
      ],
      "metadata": {
        "id": "_37JDK_CebdO"
      },
      "id": "_37JDK_CebdO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stories[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yigoqX6fXYMl",
        "outputId": "88312934-ec4b-4b8c-e07c-66c50b2b061e"
      },
      "id": "yigoqX6fXYMl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Once upon a time, there was a little boy named Timmy. Timmy loved to eat peaches. One day, he saw a peach on the ground and picked it up. But the peach was hard and not ripe yet. Timmy was sad because he couldn\\'t eat it.\\nTimmy\\'s mom saw him and said, \"Don\\'t worry, Timmy. The peach will be ripe soon. Just wait a little longer.\" Timmy waited and waited, and finally, the peach became soft and juicy. He was so happy that he could finally eat it.\\nFrom that day on, Timmy learned that good things come to those who wait. He also learned that sometimes things might seem hard or sharp, but with a little patience, they can become sweet and delicious, just like his peach.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f30df8b",
      "metadata": {
        "id": "1f30df8b"
      },
      "outputs": [],
      "source": [
        "text = \"\\n\".join(stories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e34655",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9e34655",
        "outputId": "57ff26dd-08b3-48b8-a341-8c892fdf3241"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19147122"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c596e794",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c596e794",
        "outputId": "dbeda15f-d253-4fb6-d323-8e47503557ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled \n"
          ]
        }
      ],
      "source": [
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary"
      ],
      "metadata": {
        "id": "jKea1P9hhHR4"
      },
      "id": "jKea1P9hhHR4"
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8InhsLGbFCj",
        "outputId": "63d7d296-7c62-402d-dbc1-fcf2bb9a52a3"
      },
      "id": "f8InhsLGbFCj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#$&'()*+,-./0123456789:;<?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz­´éñ ​–—‘’“”…🎓\n",
            "96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i, ch in enumerate(chars) }\n",
        "itos = {i:ch for i,ch in enumerate(chars)}"
      ],
      "metadata": {
        "id": "UHOqlwJtbKu2"
      },
      "id": "UHOqlwJtbKu2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[x] for x in l])"
      ],
      "metadata": {
        "id": "muglCKTAbNQi"
      },
      "id": "muglCKTAbNQi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc05388",
      "metadata": {
        "id": "9dc05388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a58d75a-80f1-4db2-8d3e-e40606e8a40d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19147122]) <class 'torch.Tensor'>\n",
            "tensor([ 1, 48, 71, 70, 75, 14,  1, 48, 71, 70, 75,  1, 74, 56, 78,  1, 75, 63,\n",
            "        60,  1, 74, 63, 64, 69, 80,  1, 58, 56, 73,  1, 56, 69, 59,  1, 74, 56,\n",
            "        64, 59, 12,  1,  3, 52, 70, 78, 12,  1, 40, 64, 75, 75, 80, 12,  1, 80,\n",
            "        70, 76, 73,  1, 58, 56, 73,  1, 64, 74,  1, 74, 70,  1, 57, 73, 64, 62,\n",
            "        63, 75,  1, 56, 69, 59,  1, 58, 67, 60, 56, 69,  2,  3,  1, 40, 64, 75,\n",
            "        75, 80,  1, 74, 68, 64, 67, 60, 59,  1])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, type(data))\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "I_AYpGNWhvxB"
      },
      "id": "I_AYpGNWhvxB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66922688",
      "metadata": {
        "id": "66922688"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62038547",
      "metadata": {
        "id": "62038547"
      },
      "outputs": [],
      "source": [
        "block_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a76ae5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78a76ae5",
        "outputId": "54549df9-e1a0-48ed-ea42-d6e1adfda8dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1, 48, 71, 70, 75, 14,  1, 48, 71])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd62ded",
      "metadata": {
        "id": "bfd62ded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1540f9a2-efe3-4ede-b303-41bc529671c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1, 48, 71, 70, 75, 14,  1, 48]),\n",
              " tensor([48, 71, 70, 75, 14,  1, 48, 71]))"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f98640",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2f98640",
        "outputId": "48bbcd31-376b-4176-ffe6-30413c1f67dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ctx  tensor([1]) target tensor(48)\n",
            "ctx  tensor([ 1, 48]) target tensor(71)\n",
            "ctx  tensor([ 1, 48, 71]) target tensor(70)\n",
            "ctx  tensor([ 1, 48, 71, 70]) target tensor(75)\n",
            "ctx  tensor([ 1, 48, 71, 70, 75]) target tensor(14)\n",
            "ctx  tensor([ 1, 48, 71, 70, 75, 14]) target tensor(1)\n",
            "ctx  tensor([ 1, 48, 71, 70, 75, 14,  1]) target tensor(48)\n",
            "ctx  tensor([ 1, 48, 71, 70, 75, 14,  1, 48]) target tensor(71)\n"
          ]
        }
      ],
      "source": [
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(\"ctx \", context, \"target\", target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8ab5b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d8ab5b0",
        "outputId": "0a041bf8-4684-43c1-885c-e2f30e0e2c61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff68766b4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f400257",
      "metadata": {
        "id": "4f400257"
      },
      "outputs": [],
      "source": [
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "MTBzjOo4kH0V"
      },
      "id": "MTBzjOo4kH0V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef9ed1f",
      "metadata": {
        "id": "6ef9ed1f"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed4a1c6",
      "metadata": {
        "id": "1ed4a1c6"
      },
      "outputs": [],
      "source": [
        "xb, yb = get_batch('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be4eb62",
      "metadata": {
        "id": "2be4eb62"
      },
      "outputs": [],
      "source": [
        "device= 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abafad30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abafad30",
        "outputId": "d5c05362-e604-4365-842b-2c3ed01cea15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[62, 14,  0,  0, 44, 69, 58, 60],\n",
              "        [60,  1, 76, 71, 70, 69,  1, 56],\n",
              "        [ 1, 74, 64, 67, 67, 80, 14,  1],\n",
              "        [60,  1, 78, 60, 69, 75,  1, 70]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f083be50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f083be50",
        "outputId": "b8223c0e-f1c3-4544-c43f-dc494fe8305b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([69]) ----- tensor(62)\n",
            "tensor([69, 62]) ----- tensor(14)\n",
            "tensor([69, 62, 14]) ----- tensor(0)\n",
            "tensor([69, 62, 14,  0]) ----- tensor(0)\n",
            "tensor([69, 62, 14,  0,  0]) ----- tensor(44)\n",
            "tensor([69, 62, 14,  0,  0, 44]) ----- tensor(69)\n",
            "tensor([69, 62, 14,  0,  0, 44, 69]) ----- tensor(58)\n",
            "tensor([69, 62, 14,  0,  0, 44, 69, 58]) ----- tensor(60)\n",
            "tensor([58]) ----- tensor(60)\n",
            "tensor([58, 60]) ----- tensor(1)\n",
            "tensor([58, 60,  1]) ----- tensor(76)\n",
            "tensor([58, 60,  1, 76]) ----- tensor(71)\n",
            "tensor([58, 60,  1, 76, 71]) ----- tensor(70)\n",
            "tensor([58, 60,  1, 76, 71, 70]) ----- tensor(69)\n",
            "tensor([58, 60,  1, 76, 71, 70, 69]) ----- tensor(1)\n",
            "tensor([58, 60,  1, 76, 71, 70, 69,  1]) ----- tensor(56)\n",
            "tensor([70]) ----- tensor(1)\n",
            "tensor([70,  1]) ----- tensor(74)\n",
            "tensor([70,  1, 74]) ----- tensor(64)\n",
            "tensor([70,  1, 74, 64]) ----- tensor(67)\n",
            "tensor([70,  1, 74, 64, 67]) ----- tensor(67)\n",
            "tensor([70,  1, 74, 64, 67, 67]) ----- tensor(80)\n",
            "tensor([70,  1, 74, 64, 67, 67, 80]) ----- tensor(14)\n",
            "tensor([70,  1, 74, 64, 67, 67, 80, 14]) ----- tensor(1)\n",
            "tensor([63]) ----- tensor(60)\n",
            "tensor([63, 60]) ----- tensor(1)\n",
            "tensor([63, 60,  1]) ----- tensor(78)\n",
            "tensor([63, 60,  1, 78]) ----- tensor(60)\n",
            "tensor([63, 60,  1, 78, 60]) ----- tensor(69)\n",
            "tensor([63, 60,  1, 78, 60, 69]) ----- tensor(75)\n",
            "tensor([63, 60,  1, 78, 60, 69, 75]) ----- tensor(1)\n",
            "tensor([63, 60,  1, 78, 60, 69, 75,  1]) ----- tensor(70)\n"
          ]
        }
      ],
      "source": [
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b][:t+1]\n",
        "        target = yb[b][t]\n",
        "        print(context, \"-----\", target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da57db2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9da57db2",
        "outputId": "11ecd2c1-0dd6-47a5-ce9f-d08b490812f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90997e9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90997e9e",
        "outputId": "5c55b30d-68aa-4f5d-902b-a95a1a1746ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff68766b4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1137)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-sPvPjLWkN19"
      },
      "id": "-sPvPjLWkN19"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline MOE"
      ],
      "metadata": {
        "id": "HnkfBop-p6Z6"
      },
      "id": "HnkfBop-p6Z6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gating function, returns top k experts"
      ],
      "metadata": {
        "id": "YgvIHSq9kfNh"
      },
      "id": "YgvIHSq9kfNh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc99dca5",
      "metadata": {
        "id": "cc99dca5"
      },
      "outputs": [],
      "source": [
        "class MoeLayer(nn.Module):\n",
        "    def __init__(self, experts, gate, k=1):\n",
        "        super().__init__()\n",
        "        assert len(experts) > 0\n",
        "        self.experts = nn.ModuleList(experts)\n",
        "        self.gate = gate\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        inputs_squashed = inputs.view(-1, inputs.shape[-1])\n",
        "        gate_logits = self.gate(inputs_squashed)\n",
        "        weights, selected_experts = torch.topk(\n",
        "            gate_logits, self.k\n",
        "        )\n",
        "        weights = nn.functional.softmax(\n",
        "            weights,\n",
        "            dim=1,\n",
        "            dtype=torch.float,\n",
        "        ).type_as(inputs)\n",
        "        results = torch.zeros_like(inputs_squashed)\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            batch_idx, nth_expert = torch.where(selected_experts == i)\n",
        "            results[batch_idx] += weights[batch_idx, nth_expert, None] * expert(\n",
        "                inputs_squashed[batch_idx]\n",
        "            )\n",
        "        return results.view_as(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "8Q1gTzf0kofb"
      },
      "id": "8Q1gTzf0kofb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a585da",
      "metadata": {
        "id": "b2a585da"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MulitHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(x))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4* n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "         nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.sa_head= MulitHeadAttention(n_head, n_embed//n_head)\n",
        "        self.ffw = MoeLayer(\n",
        "            experts=[FeedForward(n_embed) for _ in range(num_experts)],\n",
        "            gate=nn.Linear(n_embed, num_experts, bias=False),\n",
        "        )\n",
        "\n",
        "#       self.ffw=  FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa_head(self.ln1(x))\n",
        "        x = x+self.ffw(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed, device=device)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed, device=device)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokes):\n",
        "        for _ in range(max_new_tokes):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -132089440, :]\n",
        "            probs = F.softmax(logits, dim = -1)\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "            idx = torch.cat((idx, idx_next), dim = 1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "hKmfntElqbqr"
      },
      "id": "hKmfntElqbqr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35bc917b",
      "metadata": {
        "id": "35bc917b"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_embed = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "low_rank = True\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24122aa8",
      "metadata": {
        "id": "24122aa8"
      },
      "outputs": [],
      "source": [
        "model = Transformer()\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a31597",
      "metadata": {
        "id": "18a31597"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QtLU4WDSPoIF",
        "outputId": "05fbaf5c-593c-4001-adec-5f56081c18a9"
      },
      "id": "QtLU4WDSPoIF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba7349df",
      "metadata": {
        "id": "ba7349df"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3754ad9-9e33-4678-c93b-7ef6b1c74bcd",
        "id": "PNOQHsJ4JP29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.9518, val loss 4.9487\n",
            "step 100: train loss 4.9516, val loss 4.9505\n",
            "step 200: train loss 4.9507, val loss 4.9483\n",
            "step 300: train loss 4.9515, val loss 4.9496\n",
            "step 400: train loss 4.9507, val loss 4.9490\n",
            "step 500: train loss 4.9510, val loss 4.9495\n",
            "step 600: train loss 4.9509, val loss 4.9498\n",
            "step 700: train loss 4.9511, val loss 4.9496\n",
            "step 800: train loss 4.9516, val loss 4.9498\n",
            "step 900: train loss 4.9508, val loss 4.9493\n",
            "step 999: train loss 4.9508, val loss 4.9490\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % 100 == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.requires_grad = True\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "id": "PNOQHsJ4JP29"
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzmliCOe7PzC",
        "outputId": "c4d3b68a-9e9a-49e9-dc3f-69ce241dfe8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32089440\n"
          ]
        }
      ],
      "id": "zzmliCOe7PzC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time taken = 36min 55 sec \\\n",
        "Number of parameters = 32089440 \\\n",
        "Validation loss = 4.94 \\"
      ],
      "metadata": {
        "id": "5ycOO7KHEx15"
      },
      "id": "5ycOO7KHEx15"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Low Rank Adaptation"
      ],
      "metadata": {
        "id": "R3RYrsnmplds"
      },
      "id": "R3RYrsnmplds"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loralib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uTR2fg_LNeA",
        "outputId": "57f06f45-b5d3-40d8-a8f5-32e374e3ef78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loralib\n",
            "  Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: loralib\n",
            "Successfully installed loralib-0.1.2\n"
          ]
        }
      ],
      "id": "9uTR2fg_LNeA"
    },
    {
      "cell_type": "code",
      "source": [
        "import loralib as lora\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = lora.Linear(n_embed, head_size, r=16)\n",
        "        # nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = lora.Linear(n_embed, head_size, r=16)\n",
        "        # nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = lora.Linear(n_embed, head_size, r=16)\n",
        "        # nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MulitHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = lora.Linear(n_embed, n_embed, r=16)\n",
        "        #  nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(x))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            lora.Linear(n_embed, 4* n_embed, r=16),\n",
        "            nn.ReLU(),\n",
        "            lora.Linear(4 * n_embed, n_embed, r=16),\n",
        "         nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.sa_head= MulitHeadAttention(n_head, n_embed//n_head)\n",
        "        self.ffw = MoeLayer(\n",
        "            experts=[FeedForward(n_embed) for _ in range(num_experts)],\n",
        "            gate=lora.Linear(n_embed, num_experts, r=16),\n",
        "        )\n",
        "\n",
        "#       self.ffw=  FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa_head(self.ln1(x))\n",
        "        x = x+self.ffw(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed, device=device)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed, device=device)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_head = lora.Linear(n_embed, vocab_size, r=16)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokes):\n",
        "        for _ in range(max_new_tokes):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim = -1)\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "            idx = torch.cat((idx, idx_next), dim = 1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "d1fmLbN5LKAY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "d1fmLbN5LKAY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-EEkX_PGXlG"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_embed = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "low_rank = True\n",
        "# ------------"
      ],
      "id": "9-EEkX_PGXlG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwU-f565GXlG"
      },
      "outputs": [],
      "source": [
        "model = Transformer()\n",
        "lora.mark_only_lora_as_trainable(model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)"
      ],
      "id": "OwU-f565GXlG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXVdyfhxGXlG"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "id": "LXVdyfhxGXlG"
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "915ccd1c-a81f-4ead-da34-d7c420433ed5",
        "id": "SRUPWowzGXlG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "id": "SRUPWowzGXlG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSkqDM0tGXlG"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)"
      ],
      "id": "rSkqDM0tGXlG"
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % 100 == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7T_Nhgt8lf7",
        "outputId": "81db7e29-c838-4c45-a487-8dac1ef78833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.9131, val loss 4.9126\n",
            "step 100: train loss 3.9347, val loss 3.9342\n",
            "step 200: train loss 3.1982, val loss 3.1951\n",
            "step 300: train loss 3.0063, val loss 2.9995\n",
            "step 400: train loss 2.8888, val loss 2.8824\n",
            "step 500: train loss 2.8037, val loss 2.7952\n",
            "step 600: train loss 2.7333, val loss 2.7254\n",
            "step 700: train loss 2.6803, val loss 2.6713\n",
            "step 800: train loss 2.6309, val loss 2.6251\n",
            "step 900: train loss 2.5914, val loss 2.5806\n",
            "step 999: train loss 2.5533, val loss 2.5467\n"
          ]
        }
      ],
      "id": "-7T_Nhgt8lf7"
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VVKMSDL8swx",
        "outputId": "4a0dbaf0-9dd0-4357-f031-93764aadb85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2367360\n"
          ]
        }
      ],
      "id": "9VVKMSDL8swx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time taken = 22min 47 sec \\\n",
        "Number of parameters = 2367360 \\\n",
        "Validation loss = 2.54"
      ],
      "metadata": {
        "id": "zvfU5jFKI68m"
      },
      "id": "zvfU5jFKI68m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing model functioning"
      ],
      "metadata": {
        "id": "_hYK4IQTk6F4"
      },
      "id": "_hYK4IQTk6F4"
    },
    {
      "cell_type": "code",
      "source": [
        "d = 'once upon a time there was a '\n",
        "x = torch.tensor(encode(d), dtype = torch.long,device=device).unsqueeze(0)\n",
        "print(decode(model.generate(x, max_new_tokes=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGZjghiVcR7q",
        "outputId": "ce360a2b-c1ab-4c68-cd62-ca60872d1cac"
      },
      "id": "SGZjghiVcR7q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time there was a girl named Lily. She liked to pretend the ambulance catles: \"Can I be careful when you catch the ambulance?\" Lily was careful and protected the amaze. After her a while, her parents came by their amazing cat with a good when put in the ground. Lily realized she wanted some chest and her mom told her those about playing with the big maze they had splashed and slapped the magic toy. \n",
            "After that, she decided to open it in the living room and told her that they took the majar, she heard a voice. Gra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = 'One day'\n",
        "x = torch.tensor(encode(d), dtype = torch.long,device=device).unsqueeze(0)\n",
        "print(decode(model.generate(x, max_new_tokes=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMmn8i3SqXYH",
        "outputId": "6fa90c58-a8bd-4a0a-ee80-3b84b275149f"
      },
      "id": "ZMmn8i3SqXYH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One day, she was playing with her toys.\n",
            "While playing, she saw a big pin on the yard. It wanted to safe again looking at it. Lily wanted to help her mother, but so she talked about the pink jump and better.\n",
            "\"Wow, Lily!\" said the pink is a pink. \"What rainbows are!\"\n",
            "\"The next was safe,\" said the dog's happy steps. \"Thank you, Lily,\" said Lily.\n",
            "\"I found a big ice-cream,\" the pink grew drain.\n",
            "\"Yes, let's play!\" said Lily.\n",
            "But the warn's white was rough. \"My tooughs are looking in the sky!\" Lily smiled. Sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Training Quantization"
      ],
      "metadata": {
        "id": "Zx8iJH15no0R"
      },
      "id": "Zx8iJH15no0R"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!git clone https://github.com/mit-han-lab/smoothquant.git"
      ],
      "metadata": {
        "id": "IwbsJ9-CntHs"
      },
      "id": "IwbsJ9-CntHs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.models.opt.modeling_opt import (\n",
        "    OPTAttention,\n",
        "    OPTDecoderLayer,\n",
        "    OPTForCausalLM,\n",
        ")\n",
        "from transformers import GPT2Tokenizer\n",
        "# from content.smoothquant import smoothquant\n",
        "from smoothquant.smoothquant.smooth import smooth_lm\n",
        "from smoothquant.smoothquant.fake_quant import W8A8Linear, quantize_opt\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "cmmobD4roBZn"
      },
      "id": "cmmobD4roBZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, dataset, tokenizer, device):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        print(\"hello\")\n",
        "\n",
        "        # tokenize the dataset\n",
        "        def tokenize_function(examples):\n",
        "            example = self.tokenizer(examples[\"text\"])\n",
        "            return example\n",
        "\n",
        "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
        "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
        "        print(\"hello2\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, model):\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "        # The task is to predict the last word of the input.\n",
        "        total, hit = 0, 0\n",
        "        idx_count=0\n",
        "        print(\"hlos\")\n",
        "        for batch in tqdm(self.dataset):\n",
        "            print(idx_count)\n",
        "            input_ids = batch[\"input_ids\"].to(self.device).unsqueeze(0)\n",
        "            # print(l)\n",
        "            label = input_ids[:, -1]\n",
        "            outputs = model(input_ids)\n",
        "            print(label.shape)\n",
        "            last_token_logits = outputs.logits[:, -2, :]\n",
        "            pred = last_token_logits.argmax(dim=-1)\n",
        "            total += label.size(0)\n",
        "            hit += (pred == label).sum().item()\n",
        "            idx_count+=1\n",
        "\n",
        "        acc = hit / total\n",
        "        return acc"
      ],
      "metadata": {
        "id": "4f0BhviEoGYB"
      },
      "id": "4f0BhviEoGYB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-13b\")\n",
        "dataset = load_dataset(\"lambada\", split=\"validation[:1000]\")\n",
        "evaluator = Evaluator(dataset, tokenizer, \"cuda\")"
      ],
      "metadata": {
        "id": "Nn0S0YvtoJPw"
      },
      "id": "Nn0S0YvtoJPw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPT_FP16 Model"
      ],
      "metadata": {
        "id": "49j0GxegoMvm"
      },
      "id": "49j0GxegoMvm"
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir offload"
      ],
      "metadata": {
        "id": "Ywlxa5z7oVeE"
      },
      "id": "Ywlxa5z7oVeE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp16 = OPTForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-13b\", torch_dtype=torch.float16, device_map=\"auto\", offload_folder=\"/content/offload\"\n",
        ")"
      ],
      "metadata": {
        "id": "_IezHkCZoX2Y"
      },
      "id": "_IezHkCZoX2Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time=time.time()\n",
        "acc_fp16 = evaluator.evaluate(model_fp16)\n",
        "end_time=time.time()\n",
        "print(f\"Original model (fp16) accuracy: {acc_fp16}\")\n",
        "print(\"Time taken by original model (fp16): \", end_time-start_time, \"seconds\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd6645c-5fe7-4916-b89c-64db5520a7f5",
        "id": "XgAbeFDSre4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model (fp16) accuracy: 0.769\n",
            "Time taken by original model (fp16):  56.145999908447266 seconds\n"
          ]
        }
      ],
      "id": "XgAbeFDSre4d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPT_INT8 Model"
      ],
      "metadata": {
        "id": "CP3ZepwTp6Cc"
      },
      "id": "CP3ZepwTp6Cc"
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/smoothquant/examples/generate_act_scales.py\""
      ],
      "metadata": {
        "id": "rHlVLksZp8XV"
      },
      "id": "rHlVLksZp8XV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act_scales = torch.load(\"../act_scales/opt-13b.pt\")\n",
        "smooth_lm(model_fp16, act_scales, 0.5)\n",
        "model_smoothquant_w8a8 = quantize_opt(model_fp16)"
      ],
      "metadata": {
        "id": "S1V0ia52qC43"
      },
      "id": "S1V0ia52qC43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time=time.time()\n",
        "acc_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)\n",
        "end_time=time.time()\n",
        "print(f\"SmoothQuant W8A8 quantized model accuracy: {acc_smoothquant_w8a8}\")\n",
        "print(\"Time taken by SmoothQuant W8A8 quantized model (INT8): \", end_time-start_time, \"seconds\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838648dd-a218-448f-e227-3f7dde35bae4",
        "id": "qetiKgaFrooE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SmoothQuant W8A8 quantized model accuracy: 0.775\n",
            "Time taken by SmoothQuant W8A8 quantized model (INT8):  35.66000008583069 seconds\n"
          ]
        }
      ],
      "id": "qetiKgaFrooE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy_PCS13TpGb"
      },
      "source": [
        "# Metaflow Notebook Example\n",
        "\n",
        "## Install wandb and metaflow deps"
      ],
      "id": "wy_PCS13TpGb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xvHhXiARAMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2a5d36-8b62-479c-8ce6-6dd350b45b45"
      },
      "source": [
        "!pip install -Uqqq metaflow fastcore wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "id": "8xvHhXiARAMg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mo59L_mUDVm"
      },
      "source": [],
      "id": "2mo59L_mUDVm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X-s8tB1VNgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef398a4f-9c4c-4485-e0f2-4b30ef1fcda2"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegatruong\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ],
      "id": "_X-s8tB1VNgE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpg4yP0lTku-",
        "outputId": "ffaa9d37-464d-44cd-9426-c6890ad4424b"
      },
      "source": [
        "%%writefile test.py\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.metaflow import wandb_log\n",
        "\n",
        "from metaflow import FlowSpec, Parameter, step\n",
        "\n",
        "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "os.environ[\"METAFLOW_USER\"] = \"test_user\"\n",
        "\n",
        "\n",
        "@wandb_log(datasets=True, models=True, others=True, settings=wandb.Settings(project='metaflow_integration'))\n",
        "class WandbExampleFlowDecoClass(FlowSpec):\n",
        "    # Not obvious how to support metaflow.IncludeFile\n",
        "    seed = Parameter(\"seed\", default=1337)\n",
        "    test_size = Parameter(\"test_size\", default=0.2)\n",
        "    raw_data = Parameter(\n",
        "        \"raw_data\",\n",
        "        default=\"https://gist.githubusercontent.com/tijptjik/9408623/raw/b237fa5848349a14a14e5d4107dc7897c21951f5/wine.csv\",\n",
        "        help=\"path to the raw data\",\n",
        "    )\n",
        "\n",
        "    @step\n",
        "    def start(self):\n",
        "        self.raw_df = pd.read_csv(self.raw_data)\n",
        "        self.next(self.split_data)\n",
        "\n",
        "    @step\n",
        "    def split_data(self):\n",
        "        X = self.raw_df.drop(\"Wine\", axis=1)\n",
        "        y = self.raw_df[[\"Wine\"]]\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, y, test_size=self.test_size, random_state=self.seed\n",
        "        )\n",
        "        self.next(self.train)\n",
        "\n",
        "    @step\n",
        "    def train(self):\n",
        "        self.clf = RandomForestClassifier(random_state=self.seed)\n",
        "        self.clf.fit(self.X_train, self.y_train)\n",
        "        self.next(self.end)\n",
        "\n",
        "    @step\n",
        "    def end(self):\n",
        "        self.preds = self.clf.predict(self.X_test)\n",
        "        self.accuracy = accuracy_score(self.y_test, self.preds)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    WandbExampleFlowDecoClass()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test.py\n"
          ],
          "name": "stdout"
        }
      ],
      "id": "Jpg4yP0lTku-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQQVaKS0TmDU",
        "outputId": "a7177b42-0070-41ba-fbd1-b22d6099dad3"
      },
      "source": [
        "!python test.py --no-pylint run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[35m\u001b[1mMetaflow 2.3.5\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mWandbExampleFlowDecoClass\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:test_user\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
            "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
            "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:33.893 \u001b[0m\u001b[1mWorkflow starting (run-id 1630013373888157):\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:33.905 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:36.358 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Currently logged in as: megatruong (use `wandb login --relogin` to force relogin)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:38.964 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Tracking run with wandb version 0.12.1\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:38.964 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Syncing run splendid-puddle-1\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:38.964 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: ⭐️ View project at https://wandb.ai/megatruong/metaflow_integration\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:38.965 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: 🚀 View run at https://wandb.ai/megatruong/metaflow_integration/runs/2vgelzt0\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:38.965 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Run data is saved locally in /content/wandb/run-20210826_212936-2vgelzt0\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:38.965 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:38.965 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:39.292 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Logging artifact: raw_df (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:41.979 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Waiting for W&B process to finish, PID 1469\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:41.981 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:41.985 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Program ended successfully.\u001b[0m\n",
            "wandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:44.745 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Find user logs for this run at: /content/wandb/run-20210826_212936-2vgelzt0/logs/debug.log\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:44.745 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Find internal logs for this run at: /content/wandb/run-20210826_212936-2vgelzt0/logs/debug-internal.log\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:44.746 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:44.747 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:45.220 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[22mwandb: Synced splendid-puddle-1: https://wandb.ai/megatruong/metaflow_integration/runs/2vgelzt0\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:45.221 \u001b[0m\u001b[32m[1630013373888157/start/1 (pid 1455)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:45.231 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:47.680 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Currently logged in as: megatruong (use `wandb login --relogin` to force relogin)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.233 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Tracking run with wandb version 0.12.1\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.233 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Syncing run rose-frost-2\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.233 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: ⭐️ View project at https://wandb.ai/megatruong/metaflow_integration\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.234 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: 🚀 View run at https://wandb.ai/megatruong/metaflow_integration/runs/13abil16\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.234 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Run data is saved locally in /content/wandb/run-20210826_212947-13abil16\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.234 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.234 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.426 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Using artifact: raw_df (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.552 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Logging artifact: X_train (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.645 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Logging artifact: X_test (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.735 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Logging artifact: y_train (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:50.824 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Logging artifact: y_test (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:53.248 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:53.249 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Waiting for W&B process to finish, PID 1529\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:55.079 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Program ended successfully.\u001b[0m\n",
            "wandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:57.991 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Find user logs for this run at: /content/wandb/run-20210826_212947-13abil16/logs/debug.log\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:57.991 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Find internal logs for this run at: /content/wandb/run-20210826_212947-13abil16/logs/debug-internal.log\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:57.991 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:57.993 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:58.468 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[22mwandb: Synced rose-frost-2: https://wandb.ai/megatruong/metaflow_integration/runs/13abil16\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:58.469 \u001b[0m\u001b[32m[1630013373888157/split_data/2 (pid 1517)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:29:58.480 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:00.950 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Currently logged in as: megatruong (use `wandb login --relogin` to force relogin)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:03.494 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Tracking run with wandb version 0.12.1\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:03.495 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Syncing run dark-tree-3\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:03.495 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: ⭐️ View project at https://wandb.ai/megatruong/metaflow_integration\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:03.495 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: 🚀 View run at https://wandb.ai/megatruong/metaflow_integration/runs/34r1jt6j\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:03.495 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Run data is saved locally in /content/wandb/run-20210826_213000-34r1jt6j\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:03.495 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:03.496 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:03.848 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Using artifact: X_train (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:04.030 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Using artifact: y_train (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:04.119 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Logging artifact: clf (<class 'sklearn.ensemble._forest.RandomForestClassifier'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:06.505 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:06.505 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Waiting for W&B process to finish, PID 1595\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:06.509 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Program ended successfully.\u001b[0m\n",
            "wandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:09.671 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Find user logs for this run at: /content/wandb/run-20210826_213000-34r1jt6j/logs/debug.log\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:09.672 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Find internal logs for this run at: /content/wandb/run-20210826_213000-34r1jt6j/logs/debug-internal.log\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:09.673 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:09.673 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:10.139 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[22mwandb: Synced dark-tree-3: https://wandb.ai/megatruong/metaflow_integration/runs/34r1jt6j\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:10.140 \u001b[0m\u001b[32m[1630013373888157/train/3 (pid 1583)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:10.150 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:12.590 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Currently logged in as: megatruong (use `wandb login --relogin` to force relogin)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.142 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Tracking run with wandb version 0.12.1\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.143 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Syncing run dark-pyramid-4\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.143 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.143 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: ⭐️ View project at https://wandb.ai/megatruong/metaflow_integration\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.353 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: 🚀 View run at https://wandb.ai/megatruong/metaflow_integration/runs/1y5wrslk\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.353 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Run data is saved locally in /content/wandb/run-20210826_213012-1y5wrslk\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.353 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.354 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Using artifact: clf (<class 'sklearn.ensemble._forest.RandomForestClassifier'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.530 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Using artifact: X_test (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.732 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Using artifact: y_test (<class 'pandas.core.frame.DataFrame'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:15.820 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22m\u001b[34m\u001b[1mwandb\u001b[0m: Logging artifact: preds (<class 'numpy.ndarray'>)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:18.153 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Waiting for W&B process to finish, PID 1646\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:18.153 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22m\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:18.157 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Program ended successfully.\u001b[0m\n",
            "wandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.794 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Find user logs for this run at: /content/wandb/run-20210826_213012-1y5wrslk/logs/debug.log\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.795 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Find internal logs for this run at: /content/wandb/run-20210826_213012-1y5wrslk/logs/debug-internal.log\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.795 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Run summary:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.795 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:     accuracy 0.91667\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.795 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:     _runtime 3\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.795 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:   _timestamp 1630013415\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.795 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:        _step 0\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.796 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Run history:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.796 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:     accuracy ▁\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.796 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:     _runtime ▁\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.796 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:   _timestamp ▁\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.796 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:        _step ▁\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.796 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.796 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:20.798 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb:\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:21.275 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[22mwandb: Synced dark-pyramid-4: https://wandb.ai/megatruong/metaflow_integration/runs/1y5wrslk\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:21.277 \u001b[0m\u001b[32m[1630013373888157/end/4 (pid 1634)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
            "\u001b[35m2021-08-26 21:30:21.277 \u001b[0m\u001b[1mDone!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ],
      "id": "DQQVaKS0TmDU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}